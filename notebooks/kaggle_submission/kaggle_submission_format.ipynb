{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Open Research Dataset Challenge - What do we know about vaccines and therapuetics?\n",
    "The following questions were analysed specifically: \n",
    "- Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n",
    "  - Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n",
    "- Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n",
    "- Exploration of use of best animal models and their predictive value for a human vaccine.\n",
    "- Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n",
    "- Efforts targeted at a universal coronavirus vaccine.\n",
    "- Efforts to develop animal models and standardize challenge studies\n",
    "- Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)\n",
    "\n",
    "## Our approach - Creating a timeline visualizing the progress of vaccines/cures on COVID-19 and other similar viral diseases.\n",
    "Our goal is to create an intuitive visualization of the progress of research on vaccines and therapuetics regarding COVID-19. Not only is this useful for professional researchers in having a quick overview of the clinical trial stages of each investigated vaccine/therapeutic, but also for the public, to have a better understanding of the time frame for which to expect a cure or solution. We decided to create vizualizations of research progress of other virusses as well as COVID-19, to get a better picture of the timescale and ammount of research that goes into making a vaccine or therapeutics.\n",
    "\n",
    "Several steps were taken to create the visualizations:\n",
    "1. Load and preprocess the data:\n",
    "    - lemmatize all texts and remove stopwords\n",
    "2. Categorize papers based on keywords \n",
    "    - using either string pattern matching or word embeddings\n",
    "    - relevant words were manually selected based on the research questions and indicativaty of clinical stage trial (e.g. mouse vs human test subject, words expressing certainty etc.)\n",
    "    - categories are: virus, clinical stage, drug type\n",
    "3. Extract keywords/summaries from selected papers\n",
    "    - TODO: write how we do this @Simon, @Silvan\n",
    "5. Visualize extracted papers, links and summaries\n",
    "    - TODO: explain how (after we know how) @Levi @Gloria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.a Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# TODO: write your imports here\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "# path to data\n",
    "data_dir = '../../src'  \n",
    "keyword_dir = '../../keywords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# As kaggle only allows notebook submissions, all functions should be in the notebook. Just copy your functions and paste them here.\n",
    "          \n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load data from dataset data directory.\"\"\"\n",
    "    sha = []\n",
    "    full_text = []\n",
    "\n",
    "    subdir = [x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,x))]\n",
    "\n",
    "    print(f\"[INFO] Loading data from {data_dir}...\")\n",
    "    # loop through folders with json files\n",
    "    for folder in subdir:\n",
    "#             path = os.path.join(data_dir,folder, folder)\n",
    "        path = os.path.join(data_dir,folder, folder, 'pdf_json')\n",
    "        # loop through json files and scrape data\n",
    "        for file in os.listdir(path):\n",
    "            file_path = os.path.join(path, file)\n",
    "\n",
    "            # open file only if it is a file\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path) as f:\n",
    "                    data_json = json.load(f)\n",
    "                    sha.append(data_json['paper_id'])\n",
    "\n",
    "                    # combine abstract texts / process\n",
    "                    combined_str = ''\n",
    "                    for text in data_json['body_text']:\n",
    "                        combined_str += text['text'].lower()\n",
    "                        \n",
    "                    full_text.append(combined_str)\n",
    "\n",
    "            else:\n",
    "                print('[WARNING]', file_path, 'not a file. Check pointed path directory in load_data().')\n",
    "\n",
    "    loaded_samples = len(sha)\n",
    "    print(f\"[INFO] Data loaded into dataset instance. {loaded_samples} samples added.\")\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['sha'] = sha\n",
    "    df['full_text'] = full_text\n",
    "    \n",
    "    return df\n",
    "\n",
    "        \n",
    "def tokenize_check(text):\n",
    "    if isinstance(text, str):\n",
    "        word_tokens = word_tokenize(text)\n",
    "    elif isinstance(text, list):\n",
    "        word_tokens = text\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return word_tokens\n",
    "    \n",
    "\n",
    "def remove_stopwords(text, remove_symbols=False):\n",
    "    \"\"\" Tokenize and/or remove stopwords and/or unwanted symbols from string\"\"\"\n",
    "    list_stopwords = set(stopwords.words('english'))\n",
    "    # list of signs to be removed if parameter remove_symbols set to True\n",
    "    list_symbols = ['.', ',', '(', ')', '[', ']']\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "\n",
    "    # filter out stopwords\n",
    "    text_without_stopwords = [w for w in word_tokens if not w in list_stopwords] \n",
    "    \n",
    "    if remove_symbols is True:\n",
    "        text_without_stopwords = [w for w in text_without_stopwords if not w in list_symbols]\n",
    "    \n",
    "    return text_without_stopwords\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\" Tokenize and/or lemmatize string \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "    \n",
    "    lemmatized_text = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "def find_keywords(text, df):\n",
    "    \"\"\" Find relevant papers for the categories in df\n",
    "    Returns a dictionary with the paper id's that match the categories\n",
    "    It also stores the sentences where the matches have been found. This can be returned too if so the team decides \"\"\"\n",
    "\n",
    "    # Data cleaning:\n",
    "    # Turn df into a dictionary with a list of key phrases\n",
    "    # Lower all of them and remove null values\n",
    "    dfd = {k: [x.lower() for x in v if not pd.isnull(x)] for k, v in df.to_dict('list').items()}\n",
    "    \n",
    "    matches = {}\n",
    "    scores = {}\n",
    "    \n",
    "    # Remove redundant values (i.e., ['coronavirus', 'coronavirus disease'] can be left as ['coronavirus']; the element 'coronavirus disease' is useless)\n",
    "    for k, v in dfd.items():\n",
    "        # print(k)\n",
    "        v = [x for x in v if not any([y in x for y in [z for z in v if z != x]])]\n",
    "        dfd[k] = v\n",
    "\n",
    "        # Find matches\n",
    "        # Use the loop we're in where we've already cleaned the data to find the matches\n",
    "        for sentence in sent_tokenize(text):\n",
    "            for keyphrase in v:\n",
    "                if keyphrase in sentence:\n",
    "                    try:\n",
    "                        already_a_match = sentence in matches[k]\n",
    "                    except KeyError:\n",
    "                        matches[k] = [sentence]\n",
    "                    else:\n",
    "                        if not already_a_match:\n",
    "                            matches[k].append(sentence)\n",
    "                            \n",
    "        # score is scaled by the number of values to choose from\n",
    "        if k in matches:\n",
    "            scores[k] = len(matches)/len(v)\n",
    "\n",
    "    # return the keys with the highest score. also return the sentences for this.\n",
    "    if len(scores.keys()) > 0:\n",
    "        max_score = list(scores.keys())[np.argmax(scores.values())]\n",
    "        return max_score, matches[max_score]\n",
    "    else:\n",
    "        return 'nan','nan'\n",
    "\n",
    "def summarize(text):\n",
    "    # TODO @Simon @Silvan: extract keywords\n",
    "    return 'summary'\n",
    "\n",
    "def extract_links(data):\n",
    "    # TODO @Levi @Miguel: extract links between papers    \n",
    "    return links\n",
    "\n",
    "#def visualize_data(data,keywords,summaries):\n",
    "#    #TODO @Levi @Kwan: visualize data\n",
    "\n",
    "def apply_and_concat(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.c Relevant strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords that define the virus the paper is about (likely in title)\n",
    "virus_keywords = pd.read_csv(keyword_dir+'/virus_keywords.csv')\n",
    "\n",
    "# keywords describing clinical phase\n",
    "clinical_stage_keywords = pd.read_csv(keyword_dir+'/phase_keywords.csv')\n",
    "\n",
    "# keywords describing treatment types\n",
    "drug_keywords = pd.read_csv(keyword_dir+'/drug_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the preloaded dataframe to speed up the process\n",
    "try:\n",
    "    df = pk.load(open('df.pkl','rb'))\n",
    "except:\n",
    "    # create dataset object\n",
    "    full_texts = load_data(data_dir)\n",
    "    meta_data = pd.read_csv(data_dir+'/metadata.csv')\n",
    "\n",
    "    # merge full text and metadata, so the paper selection can be performed either on full text\n",
    "    # or abstract, if the full text is not available.\n",
    "    df = pd.merge(meta_data,full_texts,on='sha',how='outer')\n",
    "    df['full_text'][df['full_text'].isna()] = df['abstract'][df['full_text'].isna()]\n",
    "\n",
    "    # drop papers with no abstract and no full text\n",
    "    df = df.dropna(subset=['abstract','full_text'])\n",
    "    df = df[df['full_text'] != 'Unknown']\n",
    "    pk.dump(df,open('df.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8q5ondtn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cjuzul89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Epidemiology of community-acquired respiratory...</td>\n",
       "      <td>10.1016/0002-9343(85)90361-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4014285.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Upper respiratory tract infections ar...</td>\n",
       "      <td>1985-06-28</td>\n",
       "      <td>Garibaldi, Richard A.</td>\n",
       "      <td>The American Journal of Medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-9343(85)90361-4</td>\n",
       "      <td>Abstract Upper respiratory tract infections ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jhx90hh0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Monoclonal antibodies identify multiple epitop...</td>\n",
       "      <td>10.1016/0006-291x(85)91946-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2409966.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Nine hybridoma cell lines secreting a...</td>\n",
       "      <td>1985-06-28</td>\n",
       "      <td>Cherel, Isabelle; Grosclaude, Jeanne; Rouze, P...</td>\n",
       "      <td>Biochemical and Biophysical Research Communica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0006-291x(85)91946-1</td>\n",
       "      <td>Abstract Nine hybridoma cell lines secreting a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>iqswl5kh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Morphology and morphogenesis of a coronavirus ...</td>\n",
       "      <td>10.1016/0014-4800(76)90045-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187445.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The morphology and morphogenesis of v...</td>\n",
       "      <td>1976-12-31</td>\n",
       "      <td>Doughri, A.M.; Storz, J.; Hajer, I.; Fernando,...</td>\n",
       "      <td>Experimental and Molecular Pathology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0014-4800(76)90045-9</td>\n",
       "      <td>Abstract The morphology and morphogenesis of v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>z65m48tn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Demonstration of viral antigen and immunoglobu...</td>\n",
       "      <td>10.1016/0021-9975(89)90122-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2469703.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Haemagglutinating encephalomyelitis v...</td>\n",
       "      <td>1989-02-28</td>\n",
       "      <td>Narita, M.; Kawamura, H.; Haritani, M.; Kobaya...</td>\n",
       "      <td>Journal of Comparative Pathology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0021-9975(89)90122-9</td>\n",
       "      <td>Abstract Haemagglutinating encephalomyelitis v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cord_uid  sha  source_x  \\\n",
       "0   8q5ondtn  NaN  Elsevier   \n",
       "3   cjuzul89  NaN  Elsevier   \n",
       "4   jhx90hh0  NaN  Elsevier   \n",
       "15  iqswl5kh  NaN  Elsevier   \n",
       "28  z65m48tn  NaN  Elsevier   \n",
       "\n",
       "                                                title  \\\n",
       "0   Intrauterine virus infections and congenital h...   \n",
       "3   Epidemiology of community-acquired respiratory...   \n",
       "4   Monoclonal antibodies identify multiple epitop...   \n",
       "15  Morphology and morphogenesis of a coronavirus ...   \n",
       "28  Demonstration of viral antigen and immunoglobu...   \n",
       "\n",
       "                             doi pmcid  pubmed_id    license  \\\n",
       "0   10.1016/0002-8703(72)90077-4   NaN  4361535.0  els-covid   \n",
       "3   10.1016/0002-9343(85)90361-4   NaN  4014285.0  els-covid   \n",
       "4   10.1016/0006-291x(85)91946-1   NaN  2409966.0  els-covid   \n",
       "15  10.1016/0014-4800(76)90045-9   NaN   187445.0  els-covid   \n",
       "28  10.1016/0021-9975(89)90122-9   NaN  2469703.0  els-covid   \n",
       "\n",
       "                                             abstract publish_time  \\\n",
       "0   Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "3   Abstract Upper respiratory tract infections ar...   1985-06-28   \n",
       "4   Abstract Nine hybridoma cell lines secreting a...   1985-06-28   \n",
       "15  Abstract The morphology and morphogenesis of v...   1976-12-31   \n",
       "28  Abstract Haemagglutinating encephalomyelitis v...   1989-02-28   \n",
       "\n",
       "                                              authors  \\\n",
       "0                                   Overall, James C.   \n",
       "3                               Garibaldi, Richard A.   \n",
       "4   Cherel, Isabelle; Grosclaude, Jeanne; Rouze, P...   \n",
       "15  Doughri, A.M.; Storz, J.; Hajer, I.; Fernando,...   \n",
       "28  Narita, M.; Kawamura, H.; Haritani, M.; Kobaya...   \n",
       "\n",
       "                                              journal  \\\n",
       "0                              American Heart Journal   \n",
       "3                    The American Journal of Medicine   \n",
       "4   Biochemical and Biophysical Research Communica...   \n",
       "15               Experimental and Molecular Pathology   \n",
       "28                   Journal of Comparative Pathology   \n",
       "\n",
       "    Microsoft Academic Paper ID WHO #Covidence has_pdf_parse  \\\n",
       "0                           NaN            NaN         False   \n",
       "3                           NaN            NaN         False   \n",
       "4                           NaN            NaN         False   \n",
       "15                          NaN            NaN         False   \n",
       "28                          NaN            NaN         False   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0              False  custom_license   \n",
       "3              False  custom_license   \n",
       "4              False  custom_license   \n",
       "15             False  custom_license   \n",
       "28             False  custom_license   \n",
       "\n",
       "                                             url  \\\n",
       "0   https://doi.org/10.1016/0002-8703(72)90077-4   \n",
       "3   https://doi.org/10.1016/0002-9343(85)90361-4   \n",
       "4   https://doi.org/10.1016/0006-291x(85)91946-1   \n",
       "15  https://doi.org/10.1016/0014-4800(76)90045-9   \n",
       "28  https://doi.org/10.1016/0021-9975(89)90122-9   \n",
       "\n",
       "                                            full_text  \n",
       "0   Abstract The etiologic basis for the vast majo...  \n",
       "3   Abstract Upper respiratory tract infections ar...  \n",
       "4   Abstract Nine hybridoma cell lines secreting a...  \n",
       "15  Abstract The morphology and morphogenesis of v...  \n",
       "28  Abstract Haemagglutinating encephalomyelitis v...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define virus type, clinical stage and drug type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pk.load(open('df_kw.pkl','rb'))\n",
    "except:\n",
    "    # function on full text --> think about applying on full text or on abstract\n",
    "    df['virus'], df['virus_sentence'] = zip(*df['abstract'].apply(find_keywords,df=virus_keywords))\n",
    "    df['stage'], df['stage_sentence'] = zip(*df['abstract'].apply(find_keywords,df=clinical_stage_keywords))\n",
    "    df['drug'], df['drug_sentence'] = zip(*df['abstract'].apply(find_keywords,df=drug_keywords))\n",
    "    \n",
    "    # drop papers with nan values?\n",
    "    \n",
    "    pk.dump(df,open('df_kw.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summarize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['full_text'].apply(summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize extracted papers, links and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-76a95f3dae81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_data' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
